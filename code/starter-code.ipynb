{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 1: Standardized Test Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# Part 1\n",
    "\n",
    "Part 1 requires knowledge of basic Python.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new format for the SAT was released in March 2016. Since then, levels of participation in multiple states have changed with varying legislative decisions. This project aims to explore trends in SAT and ACT participation for the years 2017-2019 in two different levels. First, it seeks to identify states that have decreasing SAT participation rates and secondly, argue the change in rates of participation for three different groups of states located in a same geographic area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents:\n",
    "- [Background](#Background)\n",
    "- [Data Import & Cleaning](#Data-Import-and-Cleaning)\n",
    "- [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    "- [Data Visualization](#Visualize-the-Data)\n",
    "- [Conclusions and Recommendations](#Conclusions-and-Recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SAT and ACT are standardized tests that many colleges and universities in the United States require for their admissions process. This score is used along with other materials such as grade point average (GPA) and essay responses to determine whether or not a potential student will be accepted to the university.\n",
    "\n",
    "The SAT has two sections of the test: Evidence-Based Reading and Writing and Math ([*source*](https://www.princetonreview.com/college/sat-sections)). The ACT has 4 sections: English, Mathematics, Reading, and Science, with an additional optional writing section ([*source*](https://www.act.org/content/act/en/products-and-services/the-act/scores/understanding-your-scores.html)). They have different score ranges, which you can read more about on their websites or additional outside sources (a quick Google search will help you understand the scores for each test):\n",
    "* [SAT](https://collegereadiness.collegeboard.org/sat)\n",
    "* [ACT](https://www.act.org/content/act/en.html)\n",
    "\n",
    "Participation and scores vary from state to state. For instance, Central states are known for top scores in SAT but medium scores in ACT and the reason has been always an interesting topic especially among students from other states.The fact is that there are different approches toward standardized tests in different states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose your Data\n",
    "\n",
    "There are 10 datasets included in the [`data`](./data/) folder for this project. You are required to pick **at least two** of these to complete your analysis. Feel free to use more than two if you would like, or add other relevant datasets you find online.\n",
    "\n",
    "* [`act_2017.csv`](./data/act_2017.csv): 2017 ACT Scores by State\n",
    "* [`act_2018.csv`](./data/act_2018.csv): 2018 ACT Scores by State\n",
    "* [`act_2019.csv`](./data/act_2019.csv): 2019 ACT Scores by State\n",
    "* [`act_2019_ca.csv`](./data/act_2019_ca.csv): 2019 ACT Scores in California by School\n",
    "* [`sat_2017.csv`](./data/sat_2017.csv): 2017 SAT Scores by State\n",
    "* [`sat_2018.csv`](./data/sat_2018.csv): 2018 SAT Scores by State\n",
    "* [`sat_2019.csv`](./data/sat_2019.csv): 2019 SAT Scores by State\n",
    "* [`sat_2019_by_intended_college_major.csv`](./data/sat_2019_by_intended_college_major.csv): 2019 SAT Scores by Intended College Major\n",
    "* [`sat_2019_ca.csv`](./data/sat_2019_ca.csv): 2019 SAT Scores in California by School\n",
    "* [`sat_act_by_college.csv`](./data/sat_act_by_college.csv): Ranges of Accepted ACT & SAT Student Scores by Colleges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chosen Datasets\n",
    "* [`act_2017.csv`](./data/act_2017.csv): 2017 ACT Scores by State\n",
    "* [`act_2018.csv`](./data/act_2018.csv): 2018 ACT Scores by State\n",
    "* [`act_2019.csv`](./data/act_2019.csv): 2019 ACT Scores by State\n",
    "* [`sat_2017.csv`](./data/sat_2017.csv): 2017 SAT Scores by State\n",
    "* [`sat_2018.csv`](./data/sat_2018.csv): 2018 SAT Scores by State\n",
    "* [`sat_2019.csv`](./data/sat_2019.csv): 2019 SAT Scores by State\n",
    "\n",
    "A column of names of all states, the rate of participation and scores of each test for each state for the specific year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outside Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your problem statement and your chosen datasets, spend some time doing outside research on state policies or additional information that might be relevant. Summarize your findings below. If you bring in any outside tables or charts, make sure you are explicit about having borrowed them. If you quote any text, make sure that it renders as being quoted. **Make sure that you cite your sources.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you break down SAT scores by state, you’ll find that the Midwest outperforms the rest of the nation—and it’s not close. Illinois boasts the highest average score (1807*), while North Dakota (1799), Michigan (1782), Minnesota (1780), Missouri (1773), and Wisconsin (1771) fill out the top five. Meanwhile, Idaho finishes last (1364), while Maine (1380) and South Carolina (1436) round out the bottom three. All scores are out of 2400 and are from 2013. For reference, the average national score in 2013 was 1498. For comparison, here is heat map of SAT scores by state, where darker is a higher average score:\n",
    "![Average SAT score by state](../imgs/Average_SAT_Score_by_state_640_px.jpg)\n",
    "So the Midwest rules the SAT, but apparently, no other test. And it gets even more bizarre: the SAT is unpopular in the Midwest, where almost everyone takes the ACT instead.\n",
    "Granted, about 20% of students across America end of up taking both the SAT and ACT, but it’s much more common to choose just one exam or the other. In the Midwest, that choice is typically the ACT. Apparently, only the best students are taking the SAT in the Midwest. The ~20% of students who take both the SAT and ACT are often the most ambitious. The students who want to exhaust every possible option before turning in their college applications. Less motivated students are much more likely to take the ACT once, then put standardized testing behind them. The Midwest might win in points per test-taker, but the fact of the matter is the participation rate is less than other states.\n",
    "[source](https://www.forbes.com/sites/bentaylor/2014/07/17/why-the-midwest-dominates-the-sat/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Challenges\n",
    "\n",
    "1. Manually calculate mean:\n",
    "\n",
    "    Write a function that takes in values and returns the mean of the values. Create a list of numbers that you test on your function to check to make sure your function works!\n",
    "    \n",
    "    *Note*: Do not use any mean methods built-in to any Python libraries to do this! This should be done without importing any additional libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code: \n",
    "def calculate_mean(ls):\n",
    "    s = 0\n",
    "    for i in ls:\n",
    "        s += i\n",
    "    return s/len(ls)\n",
    "calculate_mean([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Manually calculate standard deviation:\n",
    "\n",
    "    The formula for standard deviation is below:\n",
    "\n",
    "    $$\\sigma = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu)^2}$$\n",
    "\n",
    "    Where $x_i$ represents each value in the dataset, $\\mu$ represents the mean of all values in the dataset and $n$ represents the number of values in the dataset.\n",
    "\n",
    "    Write a function that takes in values and returns the standard deviation of the values using the formula above. Hint: use the function you wrote above to calculate the mean! Use the list of numbers you created above to test on your function.\n",
    "    \n",
    "    *Note*: Do not use any standard deviation methods built-in to any Python libraries to do this! This should be done without importing any additional libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.41"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code:\n",
    "def sigma(ls):\n",
    "    s = 0\n",
    "    for i in ls:\n",
    "        s += (i - calculate_mean(ls))**2\n",
    "    return round((s/len(ls))**0.5,2)\n",
    "sigma([1,2,3,4,5]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Data cleaning function:\n",
    "    \n",
    "    Write a function that takes in a string that is a number and a percent symbol (ex. '50%', '30.5%', etc.) and converts this to a float that is the decimal approximation of the percent. For example, inputting '50%' in your function should return 0.5, '30.5%' should return 0.305, etc. Make sure to test your function to make sure it works!\n",
    "\n",
    "You will use these functions later on in the project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.305"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code:\n",
    "def clean(with_symbol):\n",
    "    return float(with_symbol[:-1])/100\n",
    "clean('30.5%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# Part 2\n",
    "\n",
    "Part 2 requires knowledge of Pandas, EDA, data cleaning, and data visualization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*All libraries used should be added here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import & Cleaning\n",
    "\n",
    "Import the datasets that you selected for this project and go through the following steps at a minimum. You are welcome to do further cleaning as you feel necessary:\n",
    "1. Display the data: print the first 5 rows of each dataframe to your Jupyter notebook.\n",
    "2. Check for missing values.\n",
    "3. Check for any obvious issues with the observations (keep in mind the minimum & maximum possible values for each test/subtest).\n",
    "4. Fix any errors you identified in steps 2-3.\n",
    "5. Display the data types of each feature.\n",
    "6. Fix any incorrect data types found in step 5.\n",
    "    - Fix any individual values preventing other columns from being the appropriate type.\n",
    "    - If your dataset has a column of percents (ex. '50%', '30.5%', etc.), use the function you wrote in Part 1 (coding challenges, number 3) to convert this to floats! *Hint*: use `.map()` or `.apply()`.\n",
    "7. Rename Columns.\n",
    "    - Column names should be all lowercase.\n",
    "    - Column names should not contain spaces (underscores will suffice--this allows for using the `df.column_name` method to access columns in addition to `df['column_name']`).\n",
    "    - Column names should be unique and informative.\n",
    "8. Drop unnecessary rows (if needed).\n",
    "9. Merge dataframes that can be merged.\n",
    "10. Perform any additional cleaning that you feel is necessary.\n",
    "11. Save your cleaned and merged dataframes as csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code:\n",
    "act_2017 = pd.read_csv('F:/GA/Projects/project-1/data/act_2017.csv')\n",
    "act_2018 = pd.read_csv('F:/GA/Projects/project-1/data/act_2018.csv')\n",
    "act_2019 = pd.read_csv('F:/GA/Projects/project-1/data/act_2019.csv')\n",
    "sat_2017 = pd.read_csv('F:/GA/Projects/project-1/data/sat_2017.csv')\n",
    "sat_2018 = pd.read_csv('F:/GA/Projects/project-1/data/sat_2018.csv')\n",
    "sat_2019 = pd.read_csv('F:/GA/Projects/project-1/data/sat_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only columns and rows I need and reassign\n",
    "act_2017 = act_2017.loc[:,['State','Participation']]\n",
    "act_2017 = act_2017.iloc[1:,:]\n",
    "act_2018 = act_2018.loc[:,['State','Participation']]\n",
    "act_2019= act_2019.loc[:,['State','Participation']]\n",
    "sat_2017 = sat_2017.loc[:,['State','Participation']]\n",
    "sat_2018 = sat_2018.loc[:,['State','Participation']]\n",
    "sat_2019 = sat_2019.loc[:,['State','Participation Rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'act' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_33856\\4287813214.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Check for missing values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mact\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'act' is not defined"
     ]
    }
   ],
   "source": [
    "#Check for missing values\n",
    "act.isna().sum()\n",
    "sat.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#types\n",
    "act_2017.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixing errors\n",
    "act_2018['State'].replace({'District of columbia':'District of Columbia'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixing errors\n",
    "act_2018.drop(19, axis=0, inplace=True)\n",
    "#df[df['column']=='__'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging dataframes 2017 and 2018\n",
    "act_1718 = pd.merge(act_2017, act_2018, how=\"outer\",on='State')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_2019.drop(51, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging 2017, 2018 and 2019\n",
    "act = pd.merge(act_1718, act_2019, how=\"outer\", on=\"State\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act.rename(columns={'Participation_x':'Participation_2017',\n",
    "                    'Participation_y':'Participation_2018',\n",
    "                    'Participation':'Participation_2019'}, \n",
    "           inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning SATs\n",
    "sat_2019.drop([39, 47], axis=0, inplace=True)\n",
    "sat_2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging SATs\n",
    "sat_1718 = pd.merge(sat_2017,sat_2018, how=\"outer\", on=\"State\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat = pd.merge(sat_1718, sat_2019, how=\"outer\", on=\"State\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "sat.rename(columns={'Participation_x':'Participation_2017','Participation_y':'Participation_2018','Participation Rate':'Participation_2019'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercase Names\n",
    "act.columns = act.columns.str.lower()\n",
    "sat.columns = sat.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act.participation_2017 = act.participation_2017.apply(clean)\n",
    "act.participation_2018 = act.participation_2018.apply(clean)\n",
    "act.participation_2019 = act.participation_2019.apply(clean)\n",
    "sat.participation_2017 = sat.participation_2017.apply(clean)\n",
    "sat.participation_2018 = sat.participation_2018.apply(clean)\n",
    "sat.participation_2019 = sat.participation_2019.apply(clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as a csv file\n",
    "act.to_csv('F:\\GA\\Projects\\project-1\\data\\ACT.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging act and sat together\n",
    "df = pd.merge(act, sat, on='state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "df.rename(columns = {'participation_2017_x':'act_participation_2017',\n",
    "                     'participation_2018_x':'act_participation_2018',\n",
    "                     'participation_2019_x':'act_participation_2019',\n",
    "                     'participation_2017_y':'sat_participation_2017',\n",
    "                     'participation_2018_y':'sat_participation_2018',\n",
    "                     'participation_2019_y':'sat_participation_2019'},\n",
    "                      inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "\n",
    "Now that we've fixed our data, and given it appropriate names, let's create a [data dictionary](http://library.ucmerced.edu/node/10249). \n",
    "\n",
    "A data dictionary provides a quick overview of features/variables/columns, alongside data types and descriptions. The more descriptive you can be, the more useful this document is.\n",
    "\n",
    "Example of a Fictional Data Dictionary Entry: \n",
    "\n",
    "|Feature|Type|Dataset|Description|\n",
    "|---|---|---|---|\n",
    "|**county_pop**|*integer*|2010 census|The population of the county (units in thousands, where 2.5 represents 2500 people).| \n",
    "|**per_poverty**|*float*|2010 census|The percent of the county over the age of 18 living below the 200% of official US poverty rate (units percent to two decimal places 98.10 means 98.1%)|\n",
    "\n",
    "[Here's a quick link to a short guide for formatting markdown in Jupyter notebooks](https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html).\n",
    "\n",
    "Provided is the skeleton for formatting a markdown table, with columns headers that will help you create a data dictionary to quickly summarize your data, as well as some examples. **This would be a great thing to copy and paste into your custom README for this project.**\n",
    "\n",
    "*Note*: if you are unsure of what a feature is, check the source of the data! This can be found in the README."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "|__Feature__|__Type__|__Dataset__|__Description__|\n",
    "|---|---|---|---|\n",
    "|__state__|object|ACT/SAT|Name of the the states|\n",
    "|act_particitation_2017|float|ACT|percentage of participation in ACT in 2017|\n",
    "|act_particitation_2018|float|ACT|percentage of participation in ACT in 2018|\n",
    "|act_particitation_2019|float|ACT|percentage of participation in ACT in 2019|\n",
    "|sat_particitation_2017|float|SAT|percentage of participation in SAT in 2017|\n",
    "|sat_particitation_2018|float|SAT|percentage of participation in SAT in 2018|\n",
    "|sat_particitation_2019|float|SAT|percentage of participation in SAT in 2019|\n",
    "|states_geo|object|ACT/SAT|'E': eastern state, 'W': western state, 'C': central state|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Complete the following steps to explore your data. You are welcome to do more EDA than the steps outlined here as you feel necessary:\n",
    "1. Summary Statistics.\n",
    "2. Use a **dictionary comprehension** to apply the standard deviation function you create in part 1 to each numeric column in the dataframe.  **No loops**.\n",
    "    - Assign the output to variable `sd` as a dictionary where: \n",
    "        - Each column name is now a key \n",
    "        - That standard deviation of the column is the value \n",
    "        - *Example Output :* `{'ACT_Math': 120, 'ACT_Reading': 120, ...}`\n",
    "3. Investigate trends in the data.\n",
    "    - Using sorting and/or masking (along with the `.head()` method to avoid printing our entire dataframe), consider questions relevant to your problem statement. Some examples are provided below (but feel free to change these questions for your specific problem):\n",
    "        - Which states have the highest and lowest participation rates for the 2017, 2019, or 2019 SAT and ACT?\n",
    "        - Which states have the highest and lowest mean total/composite scores for the 2017, 2019, or 2019 SAT and ACT?\n",
    "        - Do any states with 100% participation on a given test have a rate change year-to-year?\n",
    "        - Do any states show have >50% participation on *both* tests each year?\n",
    "        - Which colleges have the highest median SAT and ACT scores for admittance?\n",
    "        - Which California school districts have the highest and lowest mean test scores?\n",
    "    - **You should comment on your findings at each step in a markdown cell below your code block**. Make sure you include at least one example of sorting your dataframe by a column, and one example of using boolean filtering (i.e., masking) to select a subset of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2:dictionary comprehension\n",
    "#{act.column:sigma(act.column)for column in act.to_dict()}\n",
    "\n",
    "{k:sigma(v) for (k,v) in df.iloc[:,1:].to_dict('list').items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Code:\n",
    "# #Which states have the highest and lowest participation rates for the 2017, 2019, or 2019 SAT and ACT?\n",
    "# #min\n",
    "# df[df['act_participation_2017'] == min(df.act_participation_2017)]\n",
    "# #ACT min 2017: Maine\n",
    "\n",
    "# df[df['act_participation_2018'] == min(df.act_participation_2018)]\n",
    "# #ACT min 2018: Maine\n",
    "\n",
    "# df[df['act_participation_2019'] == min(df.act_participation_2019)]\n",
    "# #ACT min 2019: Maine\n",
    "\n",
    "# df[df['sat_participation_2017'] == min(df.sat_participation_2017)]\n",
    "# #SAT min 2017: Iowa, Mississippi, North Dakota\n",
    "\n",
    "# df[df['sat_participation_2018'] == min(df.sat_participation_2018)]\n",
    "# #SAT min 2018: North Dakota\n",
    "\n",
    "# df[df['sat_participation_2019'] == min(df.sat_participation_2019)]\n",
    "# #SAT min 2019: North Dakota\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which states have the highest and lowest participation rates for the 2017, 2019, or 2019 SAT and ACT?\n",
    "#min\n",
    "for col in df.columns:\n",
    "    if \"participation\" in col:   \n",
    "        s = df.loc[df[df[col] == min(df[col])].index[0]]['state']\n",
    "        print(f'min_{col}: {s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which states have the highest and lowest participation rates for the 2017, 2019, or 2019 SAT and ACT?\n",
    "#max\n",
    "for col in df.columns:\n",
    "    if \"participation\" in col:   \n",
    "        s = df.loc[df[df[col] == max(df[col])].index[0]]['state']\n",
    "        print(f'max_{col}: {s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which states have the highest and lowest participation rates for the 2017, 2019, or 2019 SAT and ACT?\n",
    "\n",
    "|__Year__|__State of Minimum Participation in ACT__|__State of Maximum Participation in ACT__|\n",
    "|---|---|---|\n",
    "|2017|Maine|Alabama|\n",
    "|2018|Maine|Alabama|\n",
    "|2019|Maine|Alabama|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "|__Year__|__State of Minimum Participation in SAT__|__State of Maximum Participation in SAT__|\n",
    "|---|---|---|\n",
    "|2017|    Iowa    |Connecticut|\n",
    "|2018|North Dakota|Colorado|\n",
    "|2019|North Dakota|Colorado|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify states that have decreasing SAT participation rates.\n",
    "df[(df['sat_participation_2017'] > df['sat_participation_2018']) & (df['sat_participation_2018'] > df['sat_participation_2019'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify states that have decreasing ACT participation rates.\n",
    "(df[(df['act_participation_2017'] > df['act_participation_2018']) & (df['act_participation_2018'] > df['act_participation_2019'])])\n",
    "#23 states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 states with decreasing ACT participation rates, and 26 states with decreasing SAT participation rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting ACT participation of 2017 for all states(sorting by a column)\n",
    "df.loc[: ,['state','act_participation_2017']].sort_values('act_participation_2017').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maine, New Hampshire and Delaware have the three lowest rates of ACT participation among all states in 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make 3 groups of eastern, westren and central states and make a new column which shows this feature for each row in column state \n",
    "east = ['Connecticut', 'Delaware', 'Florida', 'Georgia', 'Maine', 'Maryland', 'Massachusetts', 'New Hampshire', 'New York', 'New Jersey', 'North Carolina', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'Vermont', 'Virginia', 'West Virginia', 'District of Columbia']\n",
    "west = ['Alaska', 'Arizona', 'California', 'Hawaii', 'Idaho', 'Montana', 'Nevada', 'New Mexico', 'Oregon', 'Utah', 'Washington', 'Wyoming']\n",
    "center = ['North Dakota', 'South Dakota', 'Nebraska', 'Kansas', 'Oklahoma', 'Texas', 'Minnesota', 'Iowa', 'Missouri', 'Arkansas', 'Louisiana', 'Wisconsin', 'Illinois', 'Michigan', 'Indiana', 'Ohio', 'Kentucky', 'Tennessee', 'West Virginia', 'Mississippi', 'Alabama']\n",
    "print(set(east).intersection(set(center)))\n",
    "print(set(east).intersection(set(west)))\n",
    "print(set(west).intersection(set(center)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[source of westren states](https://en.wikipedia.org/wiki/Western_United_States)\n",
    "\n",
    "[source of eastern states](https://en.wikipedia.org/wiki/Eastern_United_States)\n",
    "\n",
    "[source of central states](https://en.wikipedia.org/wiki/Central_United_States)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean data\n",
    "center.remove('West Virginia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign west, east and center to each state and make a list\n",
    "states_geo = []\n",
    "\n",
    "for row in df['state']:\n",
    "    if row in east:\n",
    "        states_geo.append('E')\n",
    "    elif row in west:\n",
    "        states_geo.append('W')\n",
    "    else:\n",
    "        states_geo.append('C')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a new column\n",
    "df['states_geo'] = states_geo\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean of ACT participation rate of each geographic area for each year\n",
    "df_geo_act = df.groupby('states_geo')['act_participation_2017', 'act_participation_2018', 'act_participation_2019'].mean()\n",
    "df_geo_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean of SAT participation rate of each geographic area for each year\n",
    "df_geo_sat = df.groupby('states_geo')['sat_participation_2017', 'sat_participation_2018', 'sat_participation_2019'].mean()\n",
    "df_geo_sat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixing issues, make a column for states_geo and reassign it\n",
    "df_geo_act = df_geo_act.reset_index()\n",
    "df_geo_sat = df_geo_sat.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Data\n",
    "\n",
    "There's not a magic bullet recommendation for the right number of plots to understand a given dataset, but visualizing your data is *always* a good idea. Not only does it allow you to quickly convey your findings (even if you have a non-technical audience), it will often reveal trends in your data that escaped you when you were looking only at numbers. It is important to not only create visualizations, but to **interpret your visualizations** as well.\n",
    "\n",
    "**Every plot should**:\n",
    "- Have a title\n",
    "- Have axis labels\n",
    "- Have appropriate tick labels\n",
    "- Text is legible in a plot\n",
    "- Plots demonstrate meaningful and valid relationships\n",
    "- Have an interpretation to aid understanding\n",
    "\n",
    "Here is an example of what your plots should look like following the above guidelines. Note that while the content of this example is unrelated, the principles of visualization hold:\n",
    "\n",
    "![](https://snag.gy/hCBR1U.jpg)\n",
    "*Interpretation: The above image shows that as we increase our spending on advertising, our sales numbers also tend to increase. There is a positive correlation between advertising spending and sales.*\n",
    "\n",
    "---\n",
    "\n",
    "Here are some prompts to get you started with visualizations. Feel free to add additional visualizations as you see fit:\n",
    "1. Use Seaborn's heatmap with pandas `.corr()` to visualize correlations between all numeric features.\n",
    "    - Heatmaps are generally not appropriate for presentations, and should often be excluded from reports as they can be visually overwhelming. **However**, they can be extremely useful in identify relationships of potential interest (as well as identifying potential collinearity before modeling).\n",
    "    - Please take time to format your output, adding a title. Look through some of the additional arguments and options. (Axis labels aren't really necessary, as long as the title is informative).\n",
    "2. Visualize distributions using histograms. If you have a lot, consider writing a custom function and use subplots.\n",
    "    - *OPTIONAL*: Summarize the underlying distributions of your features (in words & statistics)\n",
    "         - Be thorough in your verbal description of these distributions.\n",
    "         - Be sure to back up these summaries with statistics.\n",
    "         - We generally assume that data we sample from a population will be normally distributed. Do we observe this trend? Explain your answers for each distribution and how you think this will affect estimates made from these data.\n",
    "3. Plot and interpret boxplots. \n",
    "    - Boxplots demonstrate central tendency and spread in variables. In a certain sense, these are somewhat redundant with histograms, but you may be better able to identify clear outliers or differences in IQR, etc.\n",
    "    - Multiple values can be plotted to a single boxplot as long as they are of the same relative scale (meaning they have similar min/max values).\n",
    "    - Each boxplot should:\n",
    "        - Only include variables of a similar scale\n",
    "        - Have clear labels for each variable\n",
    "        - Have appropriate titles and labels\n",
    "4. Plot and interpret scatter plots to view relationships between features. Feel free to write a custom function, and subplot if you'd like. Functions save both time and space.\n",
    "    - Your plots should have:\n",
    "        - Two clearly labeled axes\n",
    "        - A proper title\n",
    "        - Colors and symbols that are clear and unmistakable\n",
    "5. Additional plots of your choosing.\n",
    "    - Are there any additional trends or relationships you haven't explored? Was there something interesting you saw that you'd like to dive further into? It's likely that there are a few more plots you might want to generate to support your narrative and recommendations that you are building toward. **As always, make sure you're interpreting your plots as you go**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Does Not make sense for this dataframes and problem statement\n",
    "plt.figure(figsize = (16,9))\n",
    "corr = df.corr()\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sns.heatmap(corr, mask=mask, square=True, cmap='gist_earth_r', annot=True, vmin=-1, vmax=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.bar(df_geo_act['states_geo'], df_geo_act['act_participation_2017'], label='2017', width=0.5)\n",
    "plt.bar(df_geo_act['states_geo'], df_geo_act['act_participation_2018'], label='2018', width=0.5)\n",
    "plt.bar(df_geo_act['states_geo'], df_geo_act['act_participation_2019'], label='2019', width=0.5)\n",
    "plt.legend()\n",
    "# Create a descriptive title\n",
    "plt.title('ACT Participation Rate')\n",
    "# Add axis labels\n",
    "plt.xlabel('Central, East and West States')\n",
    "plt.ylabel('Mean of Participation Rates');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.bar(df_geo_sat['states_geo'], df_geo_sat['sat_participation_2019'], label='2019', width=0.5)\n",
    "plt.bar(df_geo_sat['states_geo'], df_geo_sat['sat_participation_2018'], label='2018', width=0.5)\n",
    "plt.bar(df_geo_sat['states_geo'], df_geo_sat['sat_participation_2017'], label='2017', width=0.5)\n",
    "plt.legend()\n",
    "# Create a descriptive title\n",
    "plt.title('SAT Participation Rate')\n",
    "# Add axis labels\n",
    "plt.xlabel('Central, East and West States')\n",
    "plt.ylabel('Mean of Participation Rates');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_geo_act['states_geo'], df_geo_act['act_participation_2017'], label='2017')\n",
    "plt.plot(df_geo_act['states_geo'], df_geo_act['act_participation_2018'], label='2018')\n",
    "plt.plot(df_geo_act['states_geo'], df_geo_act['act_participation_2019'], label='2019')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your exploration of the data, what are you key takeaways and recommendations? Make sure to answer your question of interest or address your problem statement here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To-Do:** *Edit this cell with your conclusions and recommendations.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to create your README!\n",
    "\n",
    "**To-Do:** *If you combine your problem statement, data dictionary, brief summary of your analysis, and conclusions/recommendations, you have an amazing README.md file that quickly aligns your audience to the contents of your project.* Don't forget to cite your data sources!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
